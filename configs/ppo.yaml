# Directory
log_dir: /logs
output_dir: /outputs
save_dir: /models
wandb_dir: /wandb

# Weights & Biases
project_name: "phasic-exploration--dcpg"
model_name: "PPO - Teleport RND - Warmup"

# Experiment
log_interval: 10
num_eval_episodes: 100

# Environment
distribution_mode: easy
num_levels: 200
start_level: 0
num_env_steps: 2.5e+7
normalize_reward: True

# Rollout
num_processes: 64
num_steps: 256

# Policy gradient
gamma: 0.999
gae_lambda: 0.95

# Actor-critic
actor_critic_class: PPOModel
actor_critic_params:
  shared: True

# State buffer
state_buffer_size: 16_448 
# state_buffer_size: 500_000 
prob_to_teleport: .8
teleport_warmup: 2_000_000
# teleport_warmup: 0
use_rnd: True
rnd_epoch: 1
# rnd_epoch: 0.025
buffer_type: "RND"
add_batch_size: 32 

# Agent
agent_class: PPO
agent_params:
  # PPO params
  clip_param: 0.2
  ppo_epoch: 3
  num_mini_batch: 8
  value_loss_coef: 0.5
  entropy_coef: 0.01
  lr: 5.0e-4
  eps: 1.0e-5
  max_grad_norm: 0.5
